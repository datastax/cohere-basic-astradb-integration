{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohere and Astra DB\n",
    "\n",
    "This page teaches the user how to embed text with Cohere's embedding models. That text is then indexed in an Astra DB vector store and used to find text similar to an embedded query.\n",
    "\n",
    "In this case we use a portion of the SQuAD dataset, which consists of questions and answers to set up a RAG pipeline. By embedding the questions and storing them alongside the answers in the database, and then embedding the user's query before perfoming a similarity search, we can retrieve relevant answers from the database.\n",
    "\n",
    "### Install Packages\n",
    "\n",
    "The first thing we need to do is install and import required Python packages.\n",
    "- Cohere is the interface for the cohere models. It requrired a cohere API key, which can be gotten for free at https://dashboard.cohere.com/.\n",
    "- Astrapy is the python interface for Astra DB's JSON API. It requires a database endpoint and application token, both of which can be found in the Astra UI's database overview page.\n",
    "- Datasets contians the squad dataset that we are using. It can access many datasets from the Hugging Face Datasets Hub.\n",
    "- Python-Dotenv allows the program to load the required credentials from a .env file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U cohere astrapy datasets python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from astrapy.db import AstraDB, AstraDBCollection\n",
    "from astrapy.ops import AstraDBOps\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "Now we set up our credentials and use them to create the connections to Cohere and Astra DB.\n",
    "\n",
    "- Rename the .env.template file to .env.\n",
    "- Fill the ASTRA_DB_APPLICATION_TOKEN and ASTRA_DB_API_ENDPOINT lines with the values from the Astra UI.\n",
    "- Fill the COHERE_API_KEY line with your cohere API key from the cohere dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "token = os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\")\n",
    "api_endpoint = os.getenv(\"ASTRA_DB_API_ENDPOINT\")\n",
    "cohere_key = os.getenv(\"COHERE_API_KEY\")\n",
    "\n",
    "astra_db = AstraDB(token=token, api_endpoint=api_endpoint)\n",
    "cohere = cohere.Client(cohere_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating your Astra Collection\n",
    "\n",
    "To create an Astra collection you can call the create_collection method. For a vector store application you must create your collection with a specified embedding dimension. Only embeddings of this specific length can be inserted into the collection. Therefore, we must select our Cohere embedding model first, and use that to set the embedding dimension for our collection. There is a table of Cohere embedding models and their embedding dimensions below. Set dimension to the value of your chosen model before running the below code.\n",
    "\n",
    "### Embedding Models\n",
    "| Model Name                    | Embedding Dimensions |\n",
    "|-------------------------------|----------------------|\n",
    "| embed-english-v3.0            | 1024                 |\n",
    "| embed-multilingual-v3.0       | 1024                 |\n",
    "| embed-english-light-v3.0      | 384                  |\n",
    "| embed-multilingual-light-v3.0 | 384                  |\n",
    "| embed-english-v2.0            | 4096                 |\n",
    "| embed-english-light-v2.0      | 1024                 |\n",
    "| embed-multilingual-v2.0       | 768                  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 1024\n",
    "astra_db.create_collection(collection_name=\"cohere\", dimension=dimension)\n",
    "collection = AstraDBCollection(\n",
    "    collection_name=\"cohere\", astra_db=astra_db\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "\n",
    "First we load the squad dataset. We select the first 2000 rows of the training set specifically since it contains both questions and answers. You can see an example of the questions in the dataset in the results of the next cell.\n",
    "\n",
    "Then we ask cohere for the embeddings of all of these questions. Remeber that the model selected matches the embedding dimension of the collection. When using cohere for RAG, you first embed the documents with the input_type 'search_document' and then later embed the query with the input type 'search_query'. The truncate value of 'END' means that if the provided text is too long to embed, the model will cut off the end of the offending text and return an embedding of only the beginning part.\n",
    "\n",
    "We check that the embeddings we recieve back are the correct length.\n",
    "\n",
    "Then we combine each dictionary representing a row from the squad dataset with its generated embedding. This process results in one dictionary with the squad dataset keys and values untouched and the embedding associated with the '\\$vector' key. Embeddings need to be top level values associated with the $vector key to be valid vector search targets in Astra DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad = load_dataset('squad', split='train[:2000]')\n",
    "squad[\"question\"][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = cohere.embed(\n",
    "    texts=squad[\"question\"],\n",
    "    model=\"embed-english-v3.0\",\n",
    "    input_type=\"search_document\",\n",
    "    truncate=\"END\"\n",
    "    ).embeddings\n",
    "\n",
    "len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_insert = []\n",
    "for i in range(len(squad)):\n",
    "    to_insert.append({**squad[i], \"$vector\":embeddings[i]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting the Data\n",
    "\n",
    "We use the insert_many method to insert our documents into Astra DB. We have a list of 2000 dictionaries to insert, but insert_many only takes up to 20 documents at a time. Thus the loop keeps us from running into this limitation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "i=0\n",
    "while i<(len(to_insert)):\n",
    "    res = collection.insert_many(documents=to_insert[i:(i+batch_size)])\n",
    "    print(i)\n",
    "    i=i+batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding the Query\n",
    "\n",
    "We call cohere.embed again, with the same model and truncate values, but this time with the input_type of 'search_query'. Replace the text is user_query to search for an answer to a different question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What's in front of Notre Dame?\"\n",
    "embedded_query = cohere.embed(\n",
    "    texts=[user_query],\n",
    "    model=\"embed-english-v3.0\",\n",
    "    input_type='search_query',\n",
    "    truncate='END'\n",
    ").embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Answer\n",
    "\n",
    "WE use the vector_find method to extract a limited number of rows with questions similar to the embeddded query. Then we take those rows and extract their answer value and similarity core to show the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.vector_find(embedded_query, limit=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Query: {user_query}\")\n",
    "print(\"Answers:\")\n",
    "for idx, answer in enumerate(results):\n",
    "    print(f\"\\t Answer {idx}: {answer['answers']['text']}, Score: {answer['$similarity']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
